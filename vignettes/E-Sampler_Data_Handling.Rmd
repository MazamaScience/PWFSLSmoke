---
title: "AIRSIS Data Acquisition and Handling"
author: "Rex Thompson - Mazama Science"
date: '`r Sys.Date()`'
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{AIRSIS Data Acquisition and Handling}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, echo=FALSE}
knitr::opts_chunk$set(fig.width=7, fig.height=5)
```

This document covers the basics of how the Mazama Science PWFSL Package acquires and processes raw AIRSIS data (from EBAM and ESAM monitors) to create a raw data dataframe. Relevant functions include those called by the `airsis_createRawDataFrame()` function:

* airsis_downloadData()
* airsis_parseData()
* airsis_qualityControl()
* addClustering()

## Setup

Before downloading data we need to load the PWFSLSmoke package and set up logging. You can pass
arguments to the `logger.setup()` function to send logging information to log files. In the following
example we use `logger.setLevel()` to send any ERROR output to the console.

```{r setup}
library(PWFSLSmoke)
logger.setup()
logger.setLevel(ERROR)
```



## Downloading Data

The first step in `airsis_createRawDataFrame()` function is to download the raw data for the requested monitor and time period. This is done using the `airsis_downloadData()` function. The user passes in an agency (e.g. 'USFS') and a monitor ID (e.g. '1033') and a time period.  Data is then downloaded as a raw text string from the relevant agency's AIRSIS website (e.g. <http://usfs.airsis.com>).

For example, the following code will create text strings of raw data for two different monitors over the month of September 2016.
```{r download}
fileStringEBAM <- airsis_downloadData('USFS', unitID='1012',
                                      startdate=20160901,
                                      enddate=20160930)
fileStringESAM <- airsis_downloadData('USFS', unitID='1050',
                                      startdate=20160901,
                                      enddate=20160930)
```

## Parsing Data

Once a raw data text string is in the system it is parsed and preliminarily cleaned by the `airsis_parseData()` function. Note that this function currently only supports AIRSIS EBAM and ESAM monitors. Functionality may be added in the future to parse and clean BAM1020 monitor data, as well as deprecated monitor types (e.g. EBAM 1, EBAM 2). The parsing process returns a semi-clean raw data dataframe for the monitor of interest (additional data cleaning occurs later -- see the Quality Control section below). The following summarizes the preliminary cleaning that is performed on the data during the parsing process:

* E-Sampler: It has been observed that some E-Sampler files from AIRSIS (USFS 1050) have internal rows messed up with header line information.  The parse process removes rows such as these, which can be identified by searching for '%'.
* E-Sampler: Data for UnitID=1050 in July 2016 was observed to have extra rows with some sort of metadata in columns 'Serial.Number' and 'Data.1'. We remove such rows.
* All types: Latitude, Longitude and Sys..Volts are measured at 6am and 6pm as separate GPS entries in the dataframe. These records are carried forward so they appear in all rows.

Continuing from our example above, the following code creates a dataframe for each of the two text strings created above:
```{r parse}
dfEBAM <- airsis_parseData(fileStringEBAM)
dfESAM <- airsis_parseData(fileStringESAM)
```

## Quality Control (QC)

Once the raw data is parsed and polished a bit, it is passed into the `airsis_qualityControl()` funtion, which is where the majority of the automated data cleaning takes place. A few preliminary checks are performed before the dataframe is passed into a monitor type-specific QC function. Currently, two such functions exist (i.e. `airsis_EBAMQualityControl()`, for EBAM monitors, and `airsis_EBAMQualityControl()`, for ESAM monitors). More QC functions may be added at a later date for additional monitor types.

Although separate, the functionality between the two current QC functions is nearly identical, and so they will be addressed together within the sections below. The following sub-sections provide an overview of the different elements of the QC process.

### Longitudes/Latitudes

The QC functions check monitor coordinate values against longitude and latitude limits. Default limits are as follows:

* Longitude: -180 to 180
* Latitude: -90 to 90

Any rows with coordinate values outside the limits are removed from the QC'd dataframe.

### Assigning Time

An important step in the quality control process is assigning consistent time stamps to the data so the plotting and other functions in the PWFSLSmoke package can work consistently on all data.

The EBAM and ESAM raw data includes various date/time fields. Below is a summary of the various fields, as well as our current understanding as to what each represents:

* `Date.Time.GMT` (EBAM only) - date/time of the data
* `Start.Date.Time..GMT.` (almost always data blank for both monitor types; only observed once thus far, on USFS Monitor 1012 on 9/30/16, possibly due to a monitor software version update)
* `TimeStamp` - date/time the data was received by AIRSIS
* `PDate` - date/time the data was processed by AIRSIS

For the EBAM monitors, the `Date.Time.GMT` data is always at the top of an hour, and we more or less take it at face value to represent the time associated with the data (in actuality we use the floor of the `TimeStamp` data minus an hour, though maybe we should revisit the appropriateness of this decision to protect against assigning data to the wrong time in case the data ever comes in more than an hour after the time for which it is representative). However, this nice clean `Date.Time.GMT` field is not provided for the ESAM monitors. For the ESAM monitors, we use the floor of the `TimeStamp` data, minus one hour, to assign a clean `datetime` field (see the Hourly Time Stamp Standard section below for a discussion on why we subtract one hour from both monitor types' time stamps).

While the EBAM monitors' `TimeStamp` data is typically just a few minutes after the top of an hour, this is not the case with the ESAM monitors. The ESAM monitors' `TimeStamp` data is observed to drift by a few minutes more than 60 between readings (60 is the inteval we would excpet between hourly readings), except for once a day when the readings appear to "reset" to being captured just a few minutes past the top of the hour. For example, the following plot shows the number of minutes __*after*__ the top of the hour of each `TimeStamp` row for USFS monitor ID 1049 (ESAM) in September 2016.

```{r timingPlot}
tsESAM <- lubridate::mdy_hms(dfESAM$TimeStamp)
minsESAM <- lubridate::minute(tsESAM)
plot(tsESAM, minsESAM, ylim=c(0,25), xlab="TimeStamp", ylab="TimeStamp Minute")
title("Minutes Past the Top of the Hour\nUSFS Monitor ID 1050 TimeStamp Data\nSeptember 2016")
```

Similar patterns were observed on other ESAM monitors as well. In some cases, the time offset was observed to become quite large (up to 55 minutes) before resetting; for example, see USFS Monitor ID 1049 data in September 2016. But, importantly, it appears the data offset never gets so large as to carry over to the next full hour (though it might not hurt to periodically check for this occurrence). As such, we are able to simply take the floor of the `TimeStamp` data (minus one hour) in order to assign the data to unique hours. This is the approach we have implemented in the `airsis_EBAMQualityControl()` and `airsis_ESAMQualityControl()` functions to create a new `datetime` field in the QC'd dataframe.

#### Hourly Time Stamp Standard

For both monitor types' `datetime` field, we currently subtract one hour from the floor of the reported time stamp. For example, for a reading with a time stamp of 12:04, we assign a `datetime` of 11:00. This is because the data that came in at 12:04 is (presumably) an average of the data in Hour 11. This approach, while somewhat confusing at first glance when working with raw data, makes for better clarity and consistency in downstream applications. For example, to calculate a daily average, the user can simply filter on `datetime` data that has a certain day value, and the displayed data will be representative of the data for that day.

If this approach is confusing, it might help to think about it from the perspective of years. Suppose you wanted to know the average temperature for a location over the year 2016. To find this value you would average the individual measurements for the location of interest from 1/1/16 - 12/31/16. Thus, it follows that you wouldn't be able to calculate the full year's average until the year was over; that is, the 2016 average wouldn't materialize until the beginning of 2017. But, it would be inappropriate to say that this average "belongs" to 2017. In fact, it is 2016's annual average temperature, since it consists of an average of all temperature data recorded during 2016. The same pattern can be applied to days (Tuesday's average doesn't materialize until Wednesday), and finally, to hours (Hour 11's average doesn't materialize until Hour 12).

Please note that a different approach may be warranted for different configurations, such as if the readings are representative of an instantaneous measurement at the reported time stamp (e.g. temperature at 12:04), rather than averages (e.g. average temperature from 11:00-11:59). Over time we hope to build a more thorough understanding of the meaning of each data point and timestamp so we can more appropriately label the data for comparison against itself and other data sets.

The aforementioned hourly time label standard is consistent with federal monitoring requirements (at least when working with averages).

### Data Type (EBAM Monitors Only)

The `airsis_EBAMQualityControl()` function includes a check to ensure that the `Type` field is 'PM2.5' for all rows. Nonconforming rows are removed from the QC'd dataframe.

### Missing Data and Value Checks

The QC functions screen the data for missing values and check the existing values against acceptable limits. The following table summarizes the parameters that are checked, along with default limits. When calling the QC functions on their own, the user can specify alternate limit values.

| Parameter                  | EBAM Name  | EBAM Limits      | ESAM Name     | ESAM Limits |
|:-------------:|:-----------:|:-----------:|:-----------:|:-----------:|
| Sample Flow Rate           | `Flow`     | > +/- 5% of 16.7 | `Flow.l.m.`   | Not = 2     |
| Air Temperature            | `AT`       | > 45             | `AT.C.`       | > 150       |
| Relative Humidity          | `RHi`      | > 45             | `RHi...`      | > 55        |
| Hourly PM2.5 Concentration | `ConcHr`   | > 0.984          | `Conc.mg.m3.` | > 984       |
| Time Stamp                 | `datetime` | > now            | `datetime`    | > now       |

### Duplicte Hours

The last step in the AIRSIS QC functions is to remove any records with duplicate time stamps. Occasionally, the data will include more than one record for the same time period. In many cases this is observed as two records with the same `Date.Time.GMT` (for EBAMS) / `TimeStamp` (for ESAMs) times, but different `PDate` times, which implies that the data was reprocessed (in reality, the duplicate check is based on the values in the `datetime` field). In such cases, we assume the data was reprocessed for a reason and therefore we choose to retain the entry with the latest processing time (currently we simply retain the last record as the data always appears to be in order; could rigorize with logic by incorporating processing time). All prior rows with duplicate `datetime` values are removed from the QC'd dataframe.

## Clustering

Finally, once the data is downloaded and cleaned, it is processed to assign an ID to unique deployments based on the latitude/longitude of the monitor over the period in question. The user passes in a ....

TODO:  FURTHER DEVELOP THIS SECTION...

`addClustering()`

